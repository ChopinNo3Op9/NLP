{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmwRePl_nOB4"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "In this project, I look at 2 different NLP models for classifying questions on Stack Overflow into 3 categories depending on their quality.\n",
        "\n",
        "This Case Study outlines 2 techniques to achieve the task of text classification:\n",
        "\n",
        "1.   [Training Word Embedding](https://github.com/shraddha-an/nlp/blob/main/word_embedding_classification.ipynb)\n",
        "2.   Pretrained GloVe Word Embeddings\n",
        "\n",
        "This Colab Notebook focusses on the second task, Using the GloVe pre-trained Embedding.\n",
        "\n",
        "\n",
        "**Dataset**: [Stack Overflow Questions](https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJfG7f2ok57"
      },
      "source": [
        "## 1) **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9TXLaHOnmdBY"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "# Data Manipulation/ Handling\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sb, matplotlib.pyplot as plt\n",
        "\n",
        "# NLP libraries\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P7X-jWkho0IT"
      },
      "outputs": [],
      "source": [
        "# Importing training & testing datasets\n",
        "dataset = pd.read_csv('data/train.csv')[['Body', 'Y']].rename(columns = {'Body': 'question', 'Y': 'category'})\n",
        "ds = pd.read_csv('data/valid.csv')[['Body', 'Y']].rename(columns = {'Body': 'question', 'Y': 'category'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZPixtSjpU6W"
      },
      "source": [
        "## **2) NLP Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HYw0IZIOo70V"
      },
      "outputs": [],
      "source": [
        "# Removing symbols, stopwords, punctuation\n",
        "symbols = re.compile(pattern = '[/<>(){}\\[\\]\\|@,;]')\n",
        "tags = ['href', 'http', 'https', 'www']\n",
        "\n",
        "def text_clean(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes unwanted symbols, punctuation and stop words from a given string.\n",
        "    \"\"\"\n",
        "    s = symbols.sub(' ', s)\n",
        "    for i in tags:\n",
        "        s = s.replace(i, ' ')\n",
        "    cleaned_text = ' '.join(word for word in simple_preprocess(s, deacc = True) if not word in stop_words)\n",
        "    return cleaned_text\n",
        "\n",
        "# Applying the function on the questions column\n",
        "dataset.iloc[:, 0] = dataset.iloc[:, 0].apply(text_clean)\n",
        "ds.iloc[:, 0] = ds.iloc[:, 0].apply(text_clean)\n",
        "\n",
        "# Train & Test subsets\n",
        "X_train, y_train = dataset.iloc[:, 0].values, dataset.iloc[:, 1].values.reshape(-1, 1)\n",
        "X_test, y_test = ds.iloc[:, 0].values, ds.iloc[:, 1].values.reshape(-1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aePnEQ11tokf"
      },
      "source": [
        "## **3) Categorical Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kfqV81XPqUJt"
      },
      "outputs": [],
      "source": [
        "# One Hot Encoding the Categories Column\n",
        "from sklearn.preprocessing import OneHotEncoder as ohe\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "ct = ColumnTransformer(transformers = [('one_hot_encoder', ohe(categories = 'auto'), [0])],\n",
        "                       remainder = 'passthrough')\n",
        "\n",
        "y_train = ct.fit_transform(y_train)\n",
        "y_test = ct.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWo1SnLXtzv3"
      },
      "source": [
        "## **4) Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c_BOIy-qtuP_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Steven\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Vectorizing our text corpus of questions\n",
        "# Setting some paramters\n",
        "vocab_size = 2100\n",
        "glove_dim = 50\n",
        "sequence_length = 300\n",
        "\n",
        "# Tokenization with keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tk = Tokenizer(num_words = vocab_size)\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tk.texts_to_sequences(X_train)\n",
        "X_test = tk.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding all questions with zeros\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_seq = pad_sequences(X_train, maxlen = sequence_length, padding = 'post')\n",
        "X_test_seq = pad_sequences(X_test, maxlen = sequence_length, padding = 'post')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_aJrLramGZ4"
      },
      "source": [
        "## 5) Building the Embedding Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/stanfordnlp/GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "H2jvGtNvmNgH"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'word_index' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Building the embeddings matrix out of words present in our corpus\u001b[39;00m\n\u001b[0;32m     13\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((vocab_size, glove_dim))            \u001b[38;5;66;03m# glove_dim = 50 as I chose to use the 50-D embedding; replace it with the one you choose.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, index \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_index\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m vocab_size:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
          ]
        }
      ],
      "source": [
        "# Importing the 50-dimensional embedding text file\n",
        "path = 'data/glove.6B.50d.txt'\n",
        "\n",
        "embeddings = {}\n",
        "\n",
        "with open(path, 'r', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()                                          # Each line in the file is a word + 50 integers denoting its vector.\n",
        "      embeddings[values[0]] = np.array(values[1:], 'float32')        # The first element of every line is a word & the rest 50 are its array of integers.\n",
        "\n",
        "\n",
        "# Building the embeddings matrix out of words present in our corpus\n",
        "embedding_matrix = np.zeros((vocab_size, glove_dim))            # glove_dim = 50 as I chose to use the 50-D embedding; replace it with the one you choose.\n",
        "\n",
        "for word, index in word_index.items():\n",
        "    if index < vocab_size:\n",
        "        try:\n",
        "          embedding_matrix[index] = embeddings[word]                  # If the embedding for the given word exists, retrieve it and map it to the word.\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akQ5PmeYubxT"
      },
      "source": [
        "## **6) Embedding Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOETV7ivujNm"
      },
      "outputs": [],
      "source": [
        "# Buidling & Training the NN + Embedding layer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim = vocab_size,\n",
        "                    output_dim = glove_dim,\n",
        "                    input_length = sequence_length))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 3, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
        "\n",
        "# Loading our pre-trained embedding matrix in the Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False                       # Weights won't be updated while training.\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train_seq, y_train, epochs = 20, batch_size = 512, verbose = 1)\n",
        "\n",
        "# Save the model\n",
        "#model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwj1yehSurIk"
      },
      "source": [
        "## **6) Evaluating Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9yMSuLbuqdH"
      },
      "outputs": [],
      "source": [
        "# Evaluating model performance on test set\n",
        "loss, accuracy = model.evaluate(X_test_seq, y_test, verbose = 1)\n",
        "print(\"\\nAccuracy: {}\\nLoss: {}\".format(accuracy, loss))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "pretrained_glove_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
