{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "keras-2.15.0 tensorflow-2.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmwRePl_nOB4"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "Classify on Stack Overflow into 3 categories depending on their quality.\n",
        "\n",
        "This Case Study outlines 2 techniques to achieve the task of text classification:\n",
        "\n",
        "1.   [Training Word Embedding](https://github.com/shraddha-an/nlp/blob/main/word_embedding_classification.ipynb)\n",
        "2.   Pretrained GloVe Word Embeddings\n",
        "\n",
        "This Colab Notebook focusses on the second task, Using the GloVe pre-trained Embedding.\n",
        "\n",
        "\n",
        "**Dataset**: [Stack Overflow Questions](https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJfG7f2ok57"
      },
      "source": [
        "## 1) **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9TXLaHOnmdBY"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "# Data Manipulation/ Handling\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sb, matplotlib.pyplot as plt\n",
        "\n",
        "# NLP libraries\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P7X-jWkho0IT"
      },
      "outputs": [],
      "source": [
        "# Importing training & testing datasets\n",
        "dataset = pd.read_csv('data/train.csv')[['Body', 'Y']].rename(columns = {'Body': 'question', 'Y': 'category'})\n",
        "ds = pd.read_csv('data/valid.csv')[['Body', 'Y']].rename(columns = {'Body': 'question', 'Y': 'category'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZPixtSjpU6W"
      },
      "source": [
        "## **2) Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HYw0IZIOo70V"
      },
      "outputs": [],
      "source": [
        "# Removing symbols, stopwords, punctuation\n",
        "symbols = re.compile(pattern = '[/<>(){}\\[\\]\\|@,;]')\n",
        "tags = ['href', 'http', 'https', 'www']\n",
        "\n",
        "def text_clean(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes unwanted symbols, punctuation and stop words from a given string.\n",
        "    \"\"\"\n",
        "    s = symbols.sub(' ', s)\n",
        "    for i in tags:\n",
        "        s = s.replace(i, ' ')\n",
        "    cleaned_text = ' '.join(word for word in simple_preprocess(s, deacc = True) if not word in stop_words)\n",
        "    return cleaned_text\n",
        "\n",
        "# Applying the function on the questions column\n",
        "dataset.iloc[:, 0] = dataset.iloc[:, 0].apply(text_clean)\n",
        "ds.iloc[:, 0] = ds.iloc[:, 0].apply(text_clean)\n",
        "\n",
        "# Train & Test subsets\n",
        "X_train, y_train = dataset.iloc[:, 0].values, dataset.iloc[:, 1].values.reshape(-1, 1)\n",
        "X_test, y_test = ds.iloc[:, 0].values, ds.iloc[:, 1].values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aePnEQ11tokf"
      },
      "source": [
        "## **3) Categorical Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kfqV81XPqUJt"
      },
      "outputs": [],
      "source": [
        "# One Hot Encoding the Categories Column\n",
        "from sklearn.preprocessing import OneHotEncoder as ohe\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "ct = ColumnTransformer(transformers = [('one_hot_encoder', ohe(categories = 'auto'), [0])],\n",
        "                       remainder = 'passthrough')\n",
        "\n",
        "y_train = ct.fit_transform(y_train)\n",
        "y_test = ct.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWo1SnLXtzv3"
      },
      "source": [
        "## **4) Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c_BOIy-qtuP_"
      },
      "outputs": [],
      "source": [
        "# Vectorizing our text corpus of questions\n",
        "# Setting some paramters\n",
        "vocab_size = 2100\n",
        "glove_dim = 50\n",
        "sequence_length = 300\n",
        "\n",
        "# Tokenization with keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tk = Tokenizer(num_words = vocab_size)\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tk.texts_to_sequences(X_train)\n",
        "X_test = tk.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding all questions with zeros\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_seq = pad_sequences(X_train, maxlen = sequence_length, padding = 'post')\n",
        "X_test_seq = pad_sequences(X_test, maxlen = sequence_length, padding = 'post')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_aJrLramGZ4"
      },
      "source": [
        "## **5) Building the Embedding Matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/stanfordnlp/GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H2jvGtNvmNgH"
      },
      "outputs": [],
      "source": [
        "# Importing the 50-dimensional pre-trained embedding text file\n",
        "path = 'data/glove.6B.50d.txt'\n",
        "\n",
        "embeddings = {}\n",
        "\n",
        "with open(path, 'r', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()                                          # Each line in the file is a word + 50 integers denoting its vector.\n",
        "      embeddings[values[0]] = np.array(values[1:], 'float32')        # The first element of every line is a word & the rest 50 are its array of integers.\n",
        "\n",
        "\n",
        "# Building the embeddings matrix out of words present in our corpus\n",
        "embedding_matrix = np.zeros((vocab_size, glove_dim))            # glove_dim = 50 as I chose to use the 50-D embedding; replace it with the one you choose.\n",
        "\n",
        "word_index = tk.word_index\n",
        "for word, index in word_index.items():\n",
        "    if index < vocab_size:\n",
        "        try:\n",
        "          embedding_matrix[index] = embeddings[word]                  # If the embedding for the given word exists, retrieve it and map it to the word.\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.44411999, -0.67868   , -0.094546  , ...,  0.50032997,\n",
              "         0.47428   ,  0.040882  ],\n",
              "       [-0.27987   , -0.22764   , -0.061538  , ..., -0.21626   ,\n",
              "         0.53468001,  0.15719   ],\n",
              "       ...,\n",
              "       [ 0.73847997,  0.049798  ,  0.80636001, ...,  0.2411    ,\n",
              "        -0.89866   , -0.44416001],\n",
              "       [ 0.98005998,  0.30903   , -0.87260002, ...,  0.085949  ,\n",
              "        -0.77061999,  0.117     ],\n",
              "       [-0.22017001,  0.34727001,  1.15690005, ...,  0.38277   ,\n",
              "         0.64181   , -0.78127998]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akQ5PmeYubxT"
      },
      "source": [
        "## **6) Embedding Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WOETV7ivujNm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Steven\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 50)           105000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 15000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 45003     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 150003 (585.95 KB)\n",
            "Trainable params: 150003 (585.95 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 50)           105000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 15000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 45003     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 150003 (585.95 KB)\n",
            "Trainable params: 45003 (175.79 KB)\n",
            "Non-trainable params: 105000 (410.16 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From c:\\Users\\Steven\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Steven\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "88/88 [==============================] - 3s 20ms/step - loss: 0.8946 - accuracy: 0.5710\n",
            "Epoch 2/20\n",
            "88/88 [==============================] - 2s 21ms/step - loss: 0.6530 - accuracy: 0.7110\n",
            "Epoch 3/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.5479 - accuracy: 0.7622\n",
            "Epoch 4/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.4838 - accuracy: 0.7936\n",
            "Epoch 5/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.4392 - accuracy: 0.8150\n",
            "Epoch 6/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.4045 - accuracy: 0.8327\n",
            "Epoch 7/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.3785 - accuracy: 0.8440\n",
            "Epoch 8/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.3568 - accuracy: 0.8531\n",
            "Epoch 9/20\n",
            "88/88 [==============================] - 2s 20ms/step - loss: 0.3371 - accuracy: 0.8621\n",
            "Epoch 10/20\n",
            "88/88 [==============================] - 2s 21ms/step - loss: 0.3209 - accuracy: 0.8699\n",
            "Epoch 11/20\n",
            "88/88 [==============================] - 2s 23ms/step - loss: 0.3048 - accuracy: 0.8786\n",
            "Epoch 12/20\n",
            "88/88 [==============================] - 2s 25ms/step - loss: 0.2916 - accuracy: 0.8834\n",
            "Epoch 13/20\n",
            "88/88 [==============================] - 2s 28ms/step - loss: 0.2802 - accuracy: 0.8887\n",
            "Epoch 14/20\n",
            "88/88 [==============================] - 2s 24ms/step - loss: 0.2671 - accuracy: 0.8969\n",
            "Epoch 15/20\n",
            "88/88 [==============================] - 2s 25ms/step - loss: 0.2564 - accuracy: 0.9006\n",
            "Epoch 16/20\n",
            "88/88 [==============================] - 2s 25ms/step - loss: 0.2464 - accuracy: 0.9051\n",
            "Epoch 17/20\n",
            "88/88 [==============================] - 2s 27ms/step - loss: 0.2357 - accuracy: 0.9117\n",
            "Epoch 18/20\n",
            "88/88 [==============================] - 2s 23ms/step - loss: 0.2265 - accuracy: 0.9158\n",
            "Epoch 19/20\n",
            "88/88 [==============================] - 2s 24ms/step - loss: 0.2176 - accuracy: 0.9207\n",
            "Epoch 20/20\n",
            "88/88 [==============================] - 2s 24ms/step - loss: 0.2094 - accuracy: 0.9231\n"
          ]
        }
      ],
      "source": [
        "# Buidling & Training the NN + Embedding layer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size,\n",
        "                    output_dim = glove_dim,\n",
        "                    input_length = sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 3, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
        "\n",
        "# Loading our pre-trained embedding matrix in the Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False  # Weights won't be updated while training.\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train_seq, y_train, epochs = 20, batch_size = 512, verbose = 1)\n",
        "\n",
        "# Save the model\n",
        "#model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwj1yehSurIk"
      },
      "source": [
        "## **6) Evaluating Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b9yMSuLbuqdH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.7689\n",
            "\n",
            "Accuracy: 0.7688666582107544\n",
            "Loss: 0.6589033603668213\n"
          ]
        }
      ],
      "source": [
        "# Evaluating model performance on test set\n",
        "loss, accuracy = model.evaluate(X_test_seq, y_test, verbose = 1)\n",
        "print(\"\\nAccuracy: {}\\nLoss: {}\".format(accuracy, loss))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "pretrained_glove_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
