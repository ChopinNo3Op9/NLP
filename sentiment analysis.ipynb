{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>RISING costs have forced packaging producer Hu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>Nordic Walking was first used as a summer trai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>According shipping company Viking Line , the E...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>In the building and home improvement trade , s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5842 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment\n",
       "0     The GeoSolutions technology will leverage Bene...  positive\n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3     According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4     The Swedish buyout firm has sold its remaining...   neutral\n",
       "...                                                 ...       ...\n",
       "5837  RISING costs have forced packaging producer Hu...  negative\n",
       "5838  Nordic Walking was first used as a summer trai...   neutral\n",
       "5839  According shipping company Viking Line , the E...   neutral\n",
       "5840  In the building and home improvement trade , s...   neutral\n",
       "5841  HELSINKI AFX - KCI Konecranes said it has won ...  positive\n",
       "\n",
       "[5842 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/financial sentiment.csv', header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "neutral     3130\n",
       "positive    1852\n",
       "negative     860\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a count table for the 'Sentiment' column\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Text cleaning and lemmatization\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       geosolutions technology leverage benefon gps s...\n",
       "1                             esi low bk real possibility\n",
       "2       last quarter componenta net sale doubled perio...\n",
       "3       according chamber commerce major construction ...\n",
       "4       swedish buyout firm sold remaining percent sta...\n",
       "                              ...                        \n",
       "5837    rising cost forced packaging producer huhtamak...\n",
       "5838    nordic walking first used summer training meth...\n",
       "5839    according shipping company viking line eu deci...\n",
       "5840    building home improvement trade sale decreased...\n",
       "5841    helsinki afx kci konecranes said order four ho...\n",
       "Name: processed_text, Length: 5842, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = ' '.join([word for word in nltk.word_tokenize(text) if word.isalpha()])\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatization\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "df['processed_text'] = df['Sentence'].apply(preprocess_text)\n",
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "      <td>geosolutions technology leverage benefon gps s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "      <td>esi low bk real possibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>last quarter componenta net sale doubled perio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>according chamber commerce major construction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>swedish buyout firm sold remaining percent sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>RISING costs have forced packaging producer Hu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>rising cost forced packaging producer huhtamak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>Nordic Walking was first used as a summer trai...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>nordic walking first used summer training meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>According shipping company Viking Line , the E...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>according shipping company viking line eu deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>In the building and home improvement trade , s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>building home improvement trade sale decreased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>helsinki afx kci konecranes said order four ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5842 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment  \\\n",
       "0     The GeoSolutions technology will leverage Bene...  positive   \n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative   \n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive   \n",
       "3     According to the Finnish-Russian Chamber of Co...   neutral   \n",
       "4     The Swedish buyout firm has sold its remaining...   neutral   \n",
       "...                                                 ...       ...   \n",
       "5837  RISING costs have forced packaging producer Hu...  negative   \n",
       "5838  Nordic Walking was first used as a summer trai...   neutral   \n",
       "5839  According shipping company Viking Line , the E...   neutral   \n",
       "5840  In the building and home improvement trade , s...   neutral   \n",
       "5841  HELSINKI AFX - KCI Konecranes said it has won ...  positive   \n",
       "\n",
       "                                         processed_text  \n",
       "0     geosolutions technology leverage benefon gps s...  \n",
       "1                           esi low bk real possibility  \n",
       "2     last quarter componenta net sale doubled perio...  \n",
       "3     according chamber commerce major construction ...  \n",
       "4     swedish buyout firm sold remaining percent sta...  \n",
       "...                                                 ...  \n",
       "5837  rising cost forced packaging producer huhtamak...  \n",
       "5838  nordic walking first used summer training meth...  \n",
       "5839  according shipping company viking line eu deci...  \n",
       "5840  building home improvement trade sale decreased...  \n",
       "5841  helsinki afx kci konecranes said order four ho...  \n",
       "\n",
       "[5842 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling imbalanced datasets: Upsample the minority class\n",
    "df_neutral = train_df[train_df['Sentiment'] == 'neutral']\n",
    "df_positive = train_df[train_df['Sentiment'] == 'positive']\n",
    "df_negative = train_df[train_df['Sentiment'] == 'negative']\n",
    "\n",
    "df_positive_upsampled = resample(df_positive, replace=True, n_samples=len(df_neutral), random_state=42)  # create additional copies of negative samples to balance the positive.\n",
    "df_negative_upsampled = resample(df_negative, replace=True, n_samples=len(df_neutral), random_state=42)  \n",
    "train_df_upsampled = pd.concat([df_neutral, df_positive_upsampled, df_negative_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_positive_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_negative_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>The floor area of the Yliopistonrinne project ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>floor area yliopistonrinne project sq sq ft bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>no compensation for its news , opinions or dis...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>compensation news opinion distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>This includes a EUR 39.5 mn change in the fair...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>includes eur mn change fair value investment p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>Product coverage : baked goods ; biscuits ; br...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>product coverage baked good biscuit breakfast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>The investment will be worth approximately EUR...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>investment worth approximately eur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>In January-June 2010 , diluted loss per share ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>diluted loss per share stood versus first half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>During the strike , Finnair estimates to incur...</td>\n",
       "      <td>negative</td>\n",
       "      <td>strike finnair estimate incur net loss per day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>$QCOR a little pullback is fine but if this er...</td>\n",
       "      <td>negative</td>\n",
       "      <td>qcor little pullback fine era today gain belie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>ADPnews - Jul 17 , 2009 - Finland-based steel ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>adpnews jul steel maker rautaruukki oyj ruukki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>Managing Director 's comments : `` Net sales f...</td>\n",
       "      <td>negative</td>\n",
       "      <td>managing director comment net sale first quart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7524 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment  \\\n",
       "1647  The floor area of the Yliopistonrinne project ...   neutral   \n",
       "1669  no compensation for its news , opinions or dis...   neutral   \n",
       "4577  This includes a EUR 39.5 mn change in the fair...   neutral   \n",
       "3116  Product coverage : baked goods ; biscuits ; br...   neutral   \n",
       "2764  The investment will be worth approximately EUR...   neutral   \n",
       "...                                                 ...       ...   \n",
       "153   In January-June 2010 , diluted loss per share ...  negative   \n",
       "1165  During the strike , Finnair estimates to incur...  negative   \n",
       "3214  $QCOR a little pullback is fine but if this er...  negative   \n",
       "1987  ADPnews - Jul 17 , 2009 - Finland-based steel ...  negative   \n",
       "1613  Managing Director 's comments : `` Net sales f...  negative   \n",
       "\n",
       "                                         processed_text  \n",
       "1647  floor area yliopistonrinne project sq sq ft bu...  \n",
       "1669             compensation news opinion distribution  \n",
       "4577  includes eur mn change fair value investment p...  \n",
       "3116  product coverage baked good biscuit breakfast ...  \n",
       "2764                 investment worth approximately eur  \n",
       "...                                                 ...  \n",
       "153      diluted loss per share stood versus first half  \n",
       "1165     strike finnair estimate incur net loss per day  \n",
       "3214  qcor little pullback fine era today gain belie...  \n",
       "1987  adpnews jul steel maker rautaruukki oyj ruukki...  \n",
       "1613  managing director comment net sale first quart...  \n",
       "\n",
       "[7524 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('word2vec_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingTransformer(TransformerMixin):\n",
    "    def __init__(self, word2vec):\n",
    "        if isinstance(word2vec, Word2Vec):\n",
    "            self.word2vec = word2vec.wv\n",
    "        else:\n",
    "            self.word2vec = word2vec\n",
    "        self.dim = self.word2vec.vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[word] for word in words if word in self.word2vec.key_to_index]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model):\n",
    "    # Train the model\n",
    "    model.fit(train_df_upsampled['processed_text'], train_df_upsampled['Sentiment'])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(test_df['processed_text'])\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = metrics.accuracy_score(test_df['Sentiment'], predictions)\n",
    "    precision = metrics.precision_score(test_df['Sentiment'], predictions, average='weighted')\n",
    "    recall = metrics.recall_score(test_df['Sentiment'], predictions, average='weighted')\n",
    "    f1_score = metrics.f1_score(test_df['Sentiment'], predictions, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "    classification_report = metrics.classification_report(test_df['Sentiment'], predictions)\n",
    "    print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.71\n",
      "Recall: 0.69\n",
      "F1 Score: 0.70\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.51      0.45       175\n",
      "     neutral       0.77      0.75      0.76       622\n",
      "    positive       0.75      0.67      0.71       372\n",
      "\n",
      "    accuracy                           0.69      1169\n",
      "   macro avg       0.64      0.65      0.64      1169\n",
      "weighted avg       0.71      0.69      0.70      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a text classification pipeline with both TfidfVectorizer and MeanEmbeddingTransformer\n",
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "    ])),\n",
    "    ('classifier', LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "train_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classification pipeline with both TfidfVectorizer and MeanEmbeddingTransformer\n",
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('word_embedding', MeanEmbeddingTransformer(word2vec_model)),\n",
    "    ])),\n",
    "    ('classifier', LogisticRegression(max_iter=1000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.70\n",
      "Recall: 0.69\n",
      "F1 Score: 0.69\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.47      0.42       175\n",
      "     neutral       0.77      0.75      0.76       622\n",
      "    positive       0.74      0.69      0.71       372\n",
      "\n",
      "    accuracy                           0.69      1169\n",
      "   macro avg       0.63      0.64      0.63      1169\n",
      "weighted avg       0.70      0.69      0.69      1169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('word_embedding', MeanEmbeddingTransformer(word2vec_model)),\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier(random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Precision: 0.51\n",
      "Recall: 0.54\n",
      "F1 Score: 0.50\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.11      0.06      0.08       175\n",
      "     neutral       0.57      0.81      0.67       622\n",
      "    positive       0.60      0.31      0.41       372\n",
      "\n",
      "    accuracy                           0.54      1169\n",
      "   macro avg       0.43      0.39      0.39      1169\n",
      "weighted avg       0.51      0.54      0.50      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "    ])),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "Precision: 0.67\n",
      "Recall: 0.66\n",
      "F1 Score: 0.66\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.46      0.41       175\n",
      "     neutral       0.72      0.78      0.75       622\n",
      "    positive       0.74      0.55      0.63       372\n",
      "\n",
      "    accuracy                           0.66      1169\n",
      "   macro avg       0.61      0.60      0.60      1169\n",
      "weighted avg       0.67      0.66      0.66      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classification pipeline\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())  # CountVectorizer for feature extraction and MultinomialNB (Naive Bayes) as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.70\n",
      "Recall: 0.65\n",
      "F1 Score: 0.67\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.61      0.43       175\n",
      "     neutral       0.80      0.70      0.75       622\n",
      "    positive       0.70      0.58      0.64       372\n",
      "\n",
      "    accuracy                           0.65      1169\n",
      "   macro avg       0.61      0.63      0.61      1169\n",
      "weighted avg       0.70      0.65      0.67      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classification pipeline with both TfidfVectorizer and MeanEmbeddingTransformer\n",
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('word_embedding', MeanEmbeddingTransformer(word2vec_model)),\n",
    "    ])),\n",
    "    ('classifier', SVC(kernel='linear')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer simply counts the occurrences of terms in a document.  \n",
    "TfidfVectorizer takes into account both the term frequency in the document and the inverse document frequency in the entire corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.70\n",
      "Recall: 0.68\n",
      "F1 Score: 0.69\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.49      0.42       175\n",
      "     neutral       0.76      0.74      0.75       622\n",
      "    positive       0.75      0.67      0.71       372\n",
      "\n",
      "    accuracy                           0.68      1169\n",
      "   macro avg       0.63      0.63      0.63      1169\n",
      "weighted avg       0.70      0.68      0.69      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'negative', ..., 'positive', 'positive',\n",
       "       'neutral'], dtype='<U8')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: positive\n"
     ]
    }
   ],
   "source": [
    "# Test with a new example\n",
    "new_example = [\"This feels positive.\"]\n",
    "new_example_processed = preprocess_text(new_example[0])\n",
    "predicted_label = model.predict([new_example_processed])\n",
    "print(f\"Predicted Label: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
